 The previous module was about test levels, and in this module, we will look at the so‑called test types. A test type is a group of test activities aimed at testing specific characteristics of a software system. There are two major groups, functional and nonfunctional, and we'll be looking into what kinds of testing can be performed for both. Additionally, we'll see what black‑box and white‑box testing mean and how they fit into the overall picture. Then, I'll explain what the so‑called change‑related testing is all about and what can trigger it. Finally, we'll bring the concepts of test types and test levels together to form a single unified picture.

 Functional Testing
The meaning of functional testing is in its name. We test the functionality. Does this feature function correctly? Does it work as expected or as specified? The verbs function and work can be used interchangeably here. And most of the testing you would execute to verify the behavior at any level discussed in the previous module typically falls into this category. Unit testing done by developers means asking, does this single isolated unit function correctly? Integration and system testing means asking, do the components function correctly when connected together? Then, we also have UI or user interface testing, either in the browser or on a desktop application and typically having a screen with buttons, drop‑downs, and inputs to click on. We could say that most frequently acceptance testing is executed as system and UI testing. These concepts are complimentary, not mutually exclusive. We can even say that we didn't learn anything new so far. We just placed the test levels into one category and gave it a name. But, it is useful to keep this first big category in mind as we learn about the second one in greater detail in the next clip. But before we do that, I should also mention the terms functional coverage and coverage gap. Coverage is the degree to which an application is exercised by tests expressed as a percentage. In other words, it is a concept that allows us to quantify a part of our testing process, and it gives us some level of confidence in the quality of our system. To simplify the concept to an extreme, if we had a system doing 10 things and if we had a bunch of tests that thoroughly exercised 8 out of 10 things, then we'd have 80% functional test coverage. But there's a bit more to it. Imagine a simple login page and you have to test it. What would be the coverage for it? Well, first and foremost, it depends on how many tests you can think of. If you think of just one test, enter valid credentials and verify that login was successful, then that is your 100%. And if you executed that single test, then that's 100% coverage done. After all, you use both inputs at least once. But, as you can surely tell, such definition of 100% coverage is not a guarantee of robust quality because it relies entirely on how good you are at coming up with tests. Naturally, you should think of many more tests. Invalid login, invalid password, and those can be further broken down into all sorts of inputs. Leave it empty or blank, special characters that might crash the system, and so forth. Now, this becomes the new definition of the 100%. And if you execute and verify half, that's 50%. If you execute everything, that's 100%. Developers have additional tools and techniques to define and measure the coverage of code. Two of such measures are statement coverage and decision coverage. Developers can write automated tests, run them, and the tool will tell them which parts of the code were used by the test and which ones were not. This tells the developers what kind of additional tests they can write to cover all lines and branches of code. This approach also has loopholes, but discussing this further would be outside of the scope of this course. If you are curious, however, you can take your knowledge a step further and read through this beginner‑friendly online resource that explains a variety of coverage types. Although, do be aware that such content makes use of simple code snippets and so relies on some very basic programming knowledge. The second concept I wanted to talk about is the coverage gap, and in a way it is the opposite of coverage. It is what we don't have tests for yet or you didn't execute those tests, and so you don't know if that part of the system works or not, and it is also expressed as a percentage. In our example, if we didn't set 2 out of 10 features, then we have a 20% coverage gap.

Non-functional Testing
Imagine you spend all your time on functional testing, the system works perfectly, you deliver it, and a week later, you ask the customer, does this system work properly for you? And the answer is yes, but it's really slow and it's unintuitive, and the text on the buttons is too small, oh, and we've been hacked yesterday. You could hardly call that a success, right? 

This is what non‑functional testing is about. It answers the how questions, how well, how fast, how stable, how usable, how secure, and so on. And these are the most widespread kinds of non‑functional testing, usability testing, performance testing, and security testing. Don't worry, you don't need to be an expert in all of these to consider yourself an experienced tester, in fact, it's next to impossible to master all of these. Usability is, of course, more than just saying make the text on this button bigger, but with some experience in the domain, I would argue that you could contribute to usability testing, although one of the best ways to do usability testing is via alpha and beta testing where real users give the team real feedback on what could be improved. Performance testing requires a different set of skills compared to functional testing skills. Sure, there is some overlap, but otherwise it is usually considered a specialist area. If you can design and execute both functional and performance tests, consider yourself a senior quality assurance professional. But to give you an idea of how broad performance testing is, do know that it has a lot of sub categories, among them are stress testing, load testing, and endurance testing, but you might also hear about spike testing or scalability testing. It goes without saying that such testing also requires mastery of specialized tools. Finally, security testing is a huge separate domain much bigger than performance testing. Security testing is also known as penetration testing, or pen testing, ethical hacking, or white‑hat hacking. These terms are more or less synonymous to my best knowledge. But the main point is, don't expect to become a highly skilled, functional performance, and security tester all at the same time because pen testing is simply a separate career path, it has its own certifications, and just to give you an idea of how much you have to study, I'll show you the relevant path here on Pluralsight, and it's almost 80 hours of study material just to gain the certification. We're not even talking about gaining real world experience. ISTQB does provide additional certification opportunities though. You can see over here that you can also acquire specialist knowledge for usability and performance testing, and there is also a security tester option at the advanced level. It will most likely make you a more well‑grounded tester overall, but don't take it as an equivalent of the ethical hacker certification.


Blackbox vs. Whitebox Testing
There are many ways of how we can group and categorize testing. So far, we have learned about test levels and test types. But there is at least one more popular and useful way to group them, and that is black‑box testing and white‑box testing. The box is simply an analogy to the application you're testing. And black‑box testing means that you don't look into what's inside the box. You don't know about the internals of the software. You don't need to know or care if the black box is composed of small mini boxes or modules. You rely almost entirely on requirements or documentation that describes how the box is supposed to behave in certain circumstances. Most, but not all of your activity is sending various input, letting the black box do its magic and verifying the output is correct and as expected, that would be functional black‑box testing. Of course, you might also need to test with different environment configurations, so black‑box testing is not just limited to sending input, clicking buttons, and seeing if nothing bad happens. Also, if it is part of your job, you might need to send input and time how long it takes to get the results back. It took 10 seconds, but the requirements say no more than 5. That would be a failed nonfunctional test, specifically a performance test. Integration system and acceptance testing can all be executed in a black‑box way, and it is often carried out by testers or users in case of acceptance testing. On the other hand, white‑box testing means knowing what goes inside the box and making use of that knowledge. This is most often carried out by developers in the form of unit tests. They do rely on the requirements as well, but they may also use various technical documentation to make sure the internals fit together right. Quite often, white‑box testing is also just sending input and verifying the output, it just happens on a much lower, granular level.

Change Related Testing
We have already seen so many types of testing, and you'd think that should be enough, right? To be honest with you, there are many more types of testing, because both testing and developing can be as complex as the real world itself. So I will introduce you to two more types, and then I will put them all in a structured view in the next clip. ISTQB® defines something that is called change‑related testing, and I fully agree that this must be understood. Why? Because bugs and mistakes happen. As you now know, you can find them at any stage of the lifecycle. But for the sake of keeping things simple, let's narrow this down to one of the stages where the software has already been built, the testing stage. So a bug was caught, and it needs to be fixed. The developer does that and tells you, yep, I fixed it. Here's the new version with the fix. Naturally, someone has to verify that this is the case. Not that the developer is malicious or is lying, but better be safe than sorry. And this is where testers do two kinds of change‑related testing, confirmation and regression testing. Confirmation testing answers the question, was the bug really fixed? And regression testing, did the fix or a change break anything else? Let's go into a bit more detail. Confirmation testing means verifying that something is no longer broken after the fix was delivered. It could mean that only one test failed during the test execution, or you had multiple tests fail. In any case, we should re‑execute all failed tests and make sure they all pass now. Note that we focus only on the specific area that was broken. But what often happens is a fix in one place breaks something somewhere else. It might not happen, but if it does happen, then we call it a regression. A regression happened, or there is now a regression bug. To give it a formal definition, a regression is any unintended side effect caused by a change. And this is why we should also carry out regression testing. We test the rest of this system to make sure it still works as expected, that there was no unintentional or accidental change in behavior. This might cause a problem, though. The system is likely to be big and complex, and you might have hundreds or thousands of tests. First of all, it's not a nice feeling that you suddenly have so much work, and you must retest everything again just because of a small change and just to be on the safe side. But what if? What if something did break and you'll be held responsible? And second, you're expected to be finished by the end of today, and the entire regression takes 3 days to execute manually. This is why test automation is such an important part of any modern software development lifecycle. Automated tests run hundreds or thousands of times faster. Test automation is a serious investment, but if done properly, it pays off and prevents testing from becoming a bottleneck. If you don't have automated tests to support you, then you have to prioritize and test only the most critical parts of the system. Last but not least, I should mention that a code change to fix a bug or a bug fix is a frequent kind of change, but by far not the only one. New features mean new code, and the new thing might break an old thing. Same goes for intentional changes to existing features. The customer changed their minds, and they are requesting a change. That change gets implemented, and it accidentally breaks something else. Configuration and environmental changes, different browser, different operating system, different server, different anything. Your application might not have changed, but it never exists in isolation. It relies on the ecosystem around it, and any change there might break something too.


Test Types and Test Levels
Since there are so many testing types and kinds, people get naturally confused, and I don't blame them. It takes time to wrap your head around all of this. Let me start by asking you a question. Suppose you are testing search functionality. You input a word, and you submit your search, and you expect specific results. Is this functional testing or is it black‑box testing? Wait, if this was previously broken, shouldn't we call this confirmation testing instead? Do you find this difficult to answer? If so, that's normal. The problem is the nature of the question, which gets asked a lot in one form or another. It places all of these terms in one basket and makes you choose one. It places you in either this or that, but not both mindsets, and that is hugely misleading and the reason why people have endless debates on the internet. But the answer is quite simple. It can be a mix. It can be a combination. So what should we call our testing of the search function? Well, we send some input and verify the output without knowing the internal details. So it's definitely black‑box testing. And clearly, it is also functional testing because we check that the search returns correct results. So now we are doing black‑box functional testing. But if we are reexecuting tests to make sure another change didn't break the search, then we can add regression to our long chain of objectives. Of course, you won't be using all of these terms at the same time. So if someone asks us, what are we doing right now? How should we choose the right objective? The answer is focus. What are you focusing on? If the main purpose is to check the thing is not broken any more, just say confirmation or regression testing. If there is a new database connected to the search module and you focus on the integration of the new database, then say integration testing. This tells people what you're focusing on. Even though you are doing almost the exact same activity. This brings us to the main thing that you should know for the ISTQB exam and that it's possible to perform any test type at any test level. In other words, this matrix view of testing activities is the main thing you should know for the exam. Functional testing at unit level? Can do. Performance integration testing? Yes, that makes sense. Security at system level? Yep, that can be done as well. And now I will say something from personal experience, if you'll allow me. This matrix is not perfect. It gives us a good idea that different test types and levels can be mixed and matched. But not all combinations are widespread or meaningful. For example, I don't really see performance testing done at unit level. It is possible, but the industry standard practice is to execute such testing at higher levels. On the other hand, unit usability testing is not really meaningful. Remember, unit testing is the lowest level and is done by developers. So this is not a level that users look into. So from experience, I'd say you're likely to hear system usability testing or acceptance usability testing. And if we had to try and add more kinds of testing to this view, then it would be accurate to say that black‑box testing is typically carried out at higher levels. White‑box testing is nearly synonymous with unit testing, whereas integration really sits somewhere in between, as it depends a lot on the context. And so it even has a dedicated name, gray‑box testing. For a good list of real examples of various test types that could be done at different test levels, download the official Foundation syllabus and read the end of section 2.3, test types.