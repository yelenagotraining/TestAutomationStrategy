Acceptance testing is very similar to system testing.

 Just like system testing, its objective is to validate that the delivered software is complete and it works. But not just that, there is an additional focus on who does the testing and for what purpose. 
 
 Going back to the car example, we could say, does the car work? Yes, it's great, but I ordered a minivan for my family, not a sports car. As you can see, acceptance testing is a means for customers to approve what has been delivered. 
 
 In other words, if the system does not solve the problems it was built to solve, we have not been successful. This level is often a part of closing a deal between the customer and the supplier, 
 
 and as such this is why it's so important to have clear and complete requirements. 
 
 This is what may happen in the real world. Company A pays Company B a fixed sum of a million dollars to deliver software X within a year. Company B delivers. Company A asks a couple of users to click around the system and report if it's okay. They say it's okay. The handover is complete, and just days later all sorts of problems are discovered when hundreds of people start using it. 
 
 Company A comes back to Company B and says, what the heck? Fix this immediately. And Company B has the full right to say, sorry, you signed off the handover, it's too late now. We can fix all the discovered issues, but it will cost you extra. This scenario could have been avoided if Company A hired independent testers to execute thorough acceptance testing, find most of the severe defects, throw the software back to Company B, and say, hey, we're not signing this off until you fix all of these issues, and we're not paying a dollar more. What we discussed is commonly known as UAT, user acceptance testing. The focus is on users being able to use the software for their needs. There is also OAT, operational acceptance testing, which focuses on rather boring, but necessary operations such as backup and restore, installing and uninstalling and upgrading, disaster recovery, data load and migration, as well as performance and load testing with real amounts of data and the real number of users. We typically don't think of these things until we have to do them, and when we do problems come out of nowhere. And so again, test management should be forward thinking and include such acceptance testing into their test plan.


 ### Alpha and Beta Testing
Acceptance testing can be broken down into different logical categories, specifically alpha and beta testing. You've probably heard about a beta version of some application. This is directly related. ISTQB® says that this is typically done for commercial of‑the‑shelf software, but I believe this is not necessarily the case. This can be done for just about any kind of software. In both cases, the testing is carried out by independent testers or potential or existing customers, not the testers on the development team. The main difference is that alpha testing happens at the developing organization site, while beta testing happens on the site of the customer. Or it can be an application deployed to the devices of a small group of selected users. The main point in beta testing is to use the infrastructure, both hardware and software, of the end users. You may have heard of the cliché saying, it works on my machine. This happens when things are tested on the developers' infrastructure, it works, things get delivered to the customer, they try to run it on their infrastructure, which is very likely not identical to the one developers had, and it fails. It's just impossible to predict everything, because the cause for failure might even be a small browser plugin used by the company users and that happens to interfere with your web application. Beta testing takes out the guesswork. Just try to run the system on the customer's infrastructure and see what happens.

